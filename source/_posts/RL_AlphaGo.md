# AlphaGo详解

## 简介（待完善）

基本建模是将当前棋局作为状态，然后将其当作一张二值图片（二维矩阵）

## 幼儿版本：纯Policy Network
这种本质上方法与强化学习完全无关。

方法：**将人类棋谱作为有监督的数据集**：当前局面作为Feature，将人类下出的下一步作为Target

这种方法中，没有Value，没有Reward，**本质上是在做有监督的分类和回归**

缺陷是：类似通常的有监督学习，它的表现不可能超越人类。具体表现为：它智能击败业余棋手和低段的职业棋手，无法战胜顶尖职业棋手。

这种方法也称为**模仿学习**

## 基本版：Policy Gradient

在利用有监督学习对网络做了一定的“初始化”之后

使用的Reward很简单：
- 若某一着赢得了棋局，则Reward=1
- 若某一着输了棋局，则Reward=-1
- 否则，Reward=0

也就是说**以最终胜利为目标**，而不是以胜多少目为目标。

问题：Policy Network产生的是**关于下一步动作的概率分布**，产生的动作服从该分布，**存在小概率选择不合适的着法，下出“臭棋”**。

对战顶尖选手，一步臭棋可能就会输掉比赛。

因此，依据动作分布采样不够强。

## 完整版AlphaGo Master：Policy + Value + 蒙特卡洛搜索树(MCTS)
（待完善）

蒙特卡洛搜索树：
- 围棋的搜索树机器庞大，无论是盲目搜索还是启发式搜索搞不定
- 但是我们有Policy Network和Value Network，Policy Network会产生关于动作的概率分布。因此我们可以引入**蒙特卡洛方法**
- 蒙特卡洛搜索的数学模型：
  - 设当前状态为$s$，当前动作$a$
  - 设$Q_{s,a}$是一个随即变量，表示“基于s和a进行一局游戏得到的长期收益”，则我们说的状态-动作函数就是$q(s,a) = \mathbb{E}(Q)$
  - 如果使用DFS搜索，那将要穷尽整个状态空间，统计所有叶子节点（棋局终局），然后求均值就可以求出$q(s,a)$
  - 我们将每次沿着边搜索到叶子节点（游戏结束状态）的整条搜索路径看作是**对随机变量$Q_{s,a}$的一次采样的**，然后**利用足够多的采样样本，求加权，就可以估计出$q(s,a)$**
  - 最终选择$q$值最大的动作
- 区别于以$\pi$概率分布采样，蒙特卡洛搜索通过耗费了额外的时间枚举了部分的后续游戏发展，通过显式地搜索充分考虑后续的棋局，将随机性的影响尽量降低。

